# -*- coding: utf-8 -*-
"""Copy of BulldozerPriceProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UVqef6ZtAtz2CY_y4yNmRDrHMP9Ia11R

# Predicting the sale price of the bulldozers using machine learning

## 1. Problem Definition
> How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?

## 2. Data
The data is downloaded from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data

There are 3 main datasets:

* Train.csv is the training set, which contains data through the end of 2011.
* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.
* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.

## 3. Evaluation
The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.

**Note**: The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises RMSLE.

## 4. Features
Kaggle provides a data dictionary detailing all of the features of the dataset.
https://docs.google.com/spreadsheets/d/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI/edit?usp=sharing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn

from google.colab import drive
drive.mount('/content/drive')

# Import training and validation sets
df = pd.read_csv("/content/drive/MyDrive/Mahika (1)/bluebook-for-bulldozers (1)/TrainAndValid.zip",
                 low_memory=False)

df.info()

df.isna().sum()

df.columns

fig, ax = plt.subplots()
ax.scatter(df["saledate"][:1000], df["SalePrice"][:1000]);

df.saledate[:1000]

df.saledate.dtype

df.SalePrice.plot.hist();

"""### Parsing dates
using `parse_dates`
"""

# Importing again but this time with parse_dates
df = pd.read_csv("/content/drive/MyDrive/Mahika (1)/bluebook-for-bulldozers (1)/TrainAndValid.zip",
                 low_memory=False,
                 parse_dates=["saledate"])

df.saledate.dtype

df.saledate[:1000]

fig, ax = plt.subplots()
ax.scatter(df["saledate"][:1000], df["SalePrice"][:1000]);

df.head()

"""### Sort DataFrame by saledate"""

df.sort_values(by=["saledate"], inplace=True, ascending=True)
df.saledate.head(20)

df.head()

"""### Make a copy of the original DataFrame"""

df_tmp = df.copy()

df_tmp.head(20)

df_tmp["saleYear"] = df_tmp.saledate.dt.year
df_tmp["saleMonth"] = df_tmp.saledate.dt.month
df_tmp["saleDay"] = df_tmp.saledate.dt.day
df_tmp["saleDayOfWeek"] = df_tmp.saledate.dt.dayofweek
df_tmp["saleDayOfYear"] = df_tmp.saledate.dt.dayofyear

df_tmp.head().T

df_tmp.drop("saledate", axis=1, inplace=True)

df_tmp[:1].saledate

df_tmp.state.value_counts()

len(df_tmp)

"""## 5. Modelling"""

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_jobs=-1,
                              random_state=42)

model.fit(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

df_tmp.info()

df_tmp.UsageBand.dtype

df_tmp.isna().sum()

"""### Convert string to categories"""

df_tmp.head().T

pd.api.types.is_string_dtype(df_tmp["UsageBand"])

for label, content in df_tmp.items():
    if pd.api.types.is_string_dtype(content):
        print(label)

for label, content in df_tmp.items():
    if pd.api.types.is_string_dtype(content):
        df_tmp[label] = content.astype("category").cat.as_ordered()

df_tmp.info()

df_tmp.state.cat.categories

df_tmp.state.value_counts()

df_tmp.state.cat.codes

# Check missing data
df_tmp.isnull().sum()/len(df_tmp)

"""### Save preprocessed data"""

df_tmp.to_csv("/content/drive/MyDrive/Mahika (1)/bluebook-for-bulldozers (1)/TrainAndValid.zip",
              index=False)

df_tmp = pd.read_csv("/content/drive/MyDrive/Mahika (1)/bluebook-for-bulldozers (1)/TrainAndValid.zip",
                     low_memory=False)

df_tmp.head().T

df_tmp.isna().sum()

"""## Fill missing values
### Fill numeric missing values first
"""

for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        print(label)

df_tmp.ModelID

# Check for which numeric columns have null values
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)

# Fill numeric rows with the median
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            # Add a binary column which tells us if the data was missing or not
            df_tmp[label+"_is_missing"] = pd.isnull(content)
            # Fill missing numeric values with median
            df_tmp[label] = content.fillna(content.median())

len(df_tmp)

# Check if there's any null numeric values
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)

df_tmp.isna().sum()

# Check to see how many examples were missing
df_tmp.auctioneerID_is_missing.value_counts()

"""### Filling and turning categorical variables into numbers"""

# Check for columns which aren't numeric
for label, content in df_tmp.items():
    if not pd.api.types.is_numeric_dtype(content):
        print(label)

# Turn categorical variables into numbers and fill missing
for label, content in df_tmp.items():
    if not pd.api.types.is_numeric_dtype(content):
        # Add binary column to indicate whether sample had missing value
        df_tmp[label+"_is_missing"] = pd.isnull(content)
        # Turn categories into numbers and add +1
        df_tmp[label] = pd.Categorical(content).codes+1

pd.Categorical(df_tmp["state"]).codes + 1

pd.Categorical(df_tmp["UsageBand"]).codes

df_tmp.info()

df_tmp.head().T

df_tmp.isna().sum()

df_tmp.head()

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Instantiate model
# model = RandomForestRegressor(n_jobs=-1,
#                               random_state=42)
# 
# # Fit the model
# model.fit(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

# Score the model
model.score(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

"""### Splitting data into train/validation sets"""

df_tmp.saleYear

df_tmp.saleYear.value_counts()

df_val = df_tmp[df_tmp.saleYear == 2012]
df_train = df_tmp[df_tmp.saleYear != 2012]

len(df_val), len(df_train)

# Split data into X & y
x_train, y_train = df_train.drop("SalePrice", axis=1), df_train.SalePrice
x_valid, y_valid = df_val.drop("SalePrice", axis=1), df_val.SalePrice

x_train.shape, y_train.shape, x_valid.shape, y_valid.shape

"""### Building an evaluation function"""

from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score

def rmsle(y_test, y_preds):
    """
    Caculates root mean squared log error between predictions and
    true labels.
    """
    return np.sqrt(mean_squared_log_error(y_test, y_preds))

def show_scores(model):
    train_preds = model.predict(x_train)
    val_preds = model.predict(x_valid)
    scores = {"Training MAE": mean_absolute_error(y_train, train_preds),
              "Valid MAE": mean_absolute_error(y_valid, val_preds),
              "Training RMSLE": rmsle(y_train, train_preds),
              "Valid RMSLE": rmsle(y_valid, val_preds),
              "Training R^2": r2_score(y_train, train_preds),
              "Valid R^2": r2_score(y_valid, val_preds)}
    return scores

"""### Testing model on a subset (to tune the hyperparameters)"""

# Change max_samples value
model = RandomForestRegressor(n_jobs=-1,
                              random_state=42,
                              max_samples=10000)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model.fit(x_train, y_train)

show_scores(model)

"""### Hyerparameter tuning with RandomizedSearchCV"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import RandomizedSearchCV
# 
# # Different RandomForestRegressor hyperparameters
# rf_grid = {"n_estimators": np.arange(10, 100, 10),
#            "max_depth": [None, 3, 5, 10],
#            "min_samples_split": np.arange(2, 20, 2),
#            "min_samples_leaf": np.arange(1, 20, 2),
#            "max_features": [0.5, 1, "sqrt", "auto"],
#            "max_samples": [10000]}
# 
# # Instantiate RandomizedSearchCV model
# rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,
#                                                     random_state=42),
#                               param_distributions=rf_grid,
#                               n_iter=2,
#                               cv=5,
#                               verbose=True)
# 
# # Fit the RandomizedSearchCV model
# rs_model.fit(x_train, y_train)

rs_model.best_params_

show_scores(rs_model)

"""### Train a model with the best hyperparamters"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Most ideal hyperparamters
# ideal_model = RandomForestRegressor(n_estimators=40,
#                                     min_samples_leaf=1,
#                                     min_samples_split=14,
#                                     max_features=0.5,
#                                     n_jobs=-1,
#                                     max_samples=None,
#                                     random_state=42) # random state so our results are reproducible
# 
# # Fit the ideal model
# ideal_model.fit(x_train, y_train)

show_scores(ideal_model)

"""### Make predictions on test data

"""

# Import the test data
df_test = pd.read_csv("/content/drive/MyDrive/Mahika (1)/bluebook-for-bulldozers (1)/Test.csv",
                      low_memory=False,
                      parse_dates=["saledate"])

df_test.head()

df_test.isna().sum()

"""### Preprocessing the data (getting the test dataset in the same format as our training dataset)

"""

def preprocess_data(df):
    """
    Performs transformations on df and returns transformed df.
    """
    df["saleYear"] = df.saledate.dt.year
    df["saleMonth"] = df.saledate.dt.month
    df["saleDay"] = df.saledate.dt.day
    df["saleDayOfWeek"] = df.saledate.dt.dayofweek
    df["saleDayOfYear"] = df.saledate.dt.dayofyear

    df.drop("saledate", axis=1, inplace=True)

    # Fill the numeric rows with median
    for label, content in df.items():
        if pd.api.types.is_numeric_dtype(content):
            if pd.isnull(content).sum():
                # Add a binary column which tells us if the data was missing or not
                df[label+"_is_missing"] = pd.isnull(content)
                # Fill missing numeric values with median
                df[label] = content.fillna(content.median())

        # Filled categorical missing data and turn categories into numbers
        if not pd.api.types.is_numeric_dtype(content):
            df[label+"_is_missing"] = pd.isnull(content)
            # We add +1 to the category code because pandas encodes missing categories as -1
            df[label] = pd.Categorical(content).codes+1

    return df

# Process the test data
df_test = preprocess_data(df_test)
df_test.head()

set(x_train.columns) - set(df_test.columns)

df_test["auctioneerID_is_missing"] = False
df_test.head()